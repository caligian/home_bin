#!/usr/bin/ruby

require 'fileutils'
require 'byebug'

def get_piped_stdin()
  ARGV.length.times { ARGV.shift }
  stdin = ARGF.read
  stdin.empty? ? false : stdin.chomp()
end

def exec_cmd(cmd, s)
  IO.popen(cmd, 'w') do |pipe|
    pipe.puts(s)
    pipe.close
  end
end

HELP = <<~EOF
Usage: Download urls into files using GNU Parallel

$1
- Perl s/// string. (You can use other delimiters than '/'.
  You can also chain s/// with ';'
- This is basically perl's sed

Other:
- Pass the links via stdin. Output will consist of list of filenames.
- However, it is upto the user to check if the HTMLs were really downloaded or not.
  For this purpose, the name of the directory containing files will be provided.
- Along with GNU Parallel, curl will be used. If you don't want curl, change it wget or something of your liking.
- 4 threads will be used for downloading the urls
EOF

if ARGV[0] and ARGV[0] =~ /-h(elp)?/
  puts HELP
  exit 0
end

ARGV.length == 0 and raise IOError.new("Pass some args. -h for help")

# Make a tempfile, delete it and make a tempdir of the same name and cd
current_dir = FileUtils.pwd
tempfile = %x(mktemp).chomp!
FileUtils.rm([tempfile])
FileUtils.mkdir(tempfile)
FileUtils.cd(tempfile)

# Perl sub command
sub = ARGV[0]

# Getting stdin
stdin = get_piped_stdin() or raise IOError.new("No stdin passed.")

# If only one link, don't use multithreading. For 2 use 2, For 3 use three and beyond three, 4
num_urls = stdin.scan(/\s/).length
num_threads = num_urls <= 3 ?
                num_urls == 0 ? 0 : num_urls
              : 3
thread_s = num_threads == 0 ? "" : "-j #{num_threads}"

# Write to command
sub = %("{= #{sub} =}")

# Using curl
# parallel_cmd = %(parallel curl {} -o #{sub} -q :::: -)

# Using aria2c
parallel_cmd = %(parallel #{thread_s} aria2c  {} -o #{sub} -q :::: -)
exec_cmd(parallel_cmd, stdin)

# ls this to get the downloaded files.
puts tempfile
